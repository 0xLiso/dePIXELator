{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  !conda install -y pillow\n",
    "#  !conda install -y pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pathlib\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.list_physical_devices('GPU') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do we need this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        try:\n",
    "            # Currently, memory growth needs to be the same across GPUs\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "            print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "        except RuntimeError as e:\n",
    "            # Memory growth must be set before GPUs have been initialized\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define some base vars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_height = 180\n",
    "img_width = 320\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "\n",
    "train_path = \"data/nuberoja/train\"\n",
    "val_path = \"data/nuberoja/validation\"\n",
    "\n",
    "\n",
    "if tf.test.is_built_with_cuda():\n",
    "    data_format = 'channels_first'\n",
    "    tf.keras.backend.set_image_data_format('channels_first')\n",
    "else:\n",
    "    data_format = 'channels_last'\n",
    "    tf.keras.backend.set_image_data_format('channels_last')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic autoencoder just to test the dePIXELator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_format_shape=(3,img_height,img_width)\n",
    "if data_format!='channels_first':\n",
    "    data_format_shape=(img_height,img_width,3)\n",
    "input_img = tf.keras.layers.Input(shape=data_format_shape)  #source res channels_first for cuda+cudnn\n",
    "\n",
    "l1 = tf.keras.layers.Conv2D(64, (3, 3), padding='same', kernel_initializer='he_uniform', activation='relu', activity_regularizer=tf.keras.regularizers.l2(10e-10))(input_img)\n",
    "l2 = tf.keras.layers.Conv2D(64, (3, 3), padding='same', kernel_initializer='he_uniform', activation='relu', activity_regularizer=tf.keras.regularizers.l2(10e-10))(l1)\n",
    "l3 = tf.keras.layers.MaxPool2D(padding='same')(l2)\n",
    "\n",
    "l4 = tf.keras.layers.Conv2D(128, (3, 3), padding='same', kernel_initializer='he_uniform', activation='relu', activity_regularizer=tf.keras.regularizers.l2(10e-10))(l3)\n",
    "l5 = tf.keras.layers.Conv2D(128, (3, 3), padding='same', kernel_initializer='he_uniform', activation='relu', activity_regularizer=tf.keras.regularizers.l2(10e-10))(l4)\n",
    "l6 = tf.keras.layers.MaxPool2D(padding='same')(l5)\n",
    "\n",
    "l7 = tf.keras.layers.Conv2D(256, (3, 3), padding='same', kernel_initializer='he_uniform', activation='relu', activity_regularizer=tf.keras.regularizers.l2(10e-10))(l6)\n",
    "\n",
    "l8 = tf.keras.layers.UpSampling2D()(l7)\n",
    "l9 = tf.keras.layers.Conv2D(128, (3, 3), padding='same', kernel_initializer='he_uniform', activation='relu', activity_regularizer=tf.keras.regularizers.l2(10e-10))(l8)\n",
    "l10 = tf.keras.layers.Conv2D(128, (3, 3), padding='same', kernel_initializer='he_uniform', activation='relu', activity_regularizer=tf.keras.regularizers.l2(10e-10))(l9)\n",
    "\n",
    "l11 = tf.keras.layers.add([l10, l5])\n",
    "\n",
    "l12 = tf.keras.layers.UpSampling2D()(l11)\n",
    "l13 = tf.keras.layers.Conv2D(64, (3, 3), padding='same', kernel_initializer='he_uniform', activation='relu', activity_regularizer=tf.keras.regularizers.l2(10e-10))(l12)\n",
    "l14 = tf.keras.layers.Conv2D(64, (3, 3), padding='same', kernel_initializer='he_uniform', activation='relu', activity_regularizer=tf.keras.regularizers.l2(10e-10))(l13)\n",
    "\n",
    "l15 = tf.keras.layers.add([l14, l2])\n",
    "\n",
    "decoded_image = tf.keras.layers.Conv2D(3, (3, 3), padding='same', kernel_initializer='he_uniform', activation='relu', activity_regularizer=tf.keras.regularizers.l2(10e-10))(l15)\n",
    "\n",
    "auto_encoder = tf.keras.models.Model(inputs=(input_img), outputs=decoded_image)\n",
    "\n",
    "auto_encoder.compile(optimizer='Adam', loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from batchgenV2 import DePIXELatorBatchGenV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_gen = DePIXELatorBatchGenV2(directory_high_res=train_path,\n",
    "                 image_size_high=(img_height, img_width),\n",
    "                 batch_size=batch_size,\n",
    "                 shuffle=True,\n",
    "                 seed=42,\n",
    "                 data_format=data_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validator_gen = DePIXELatorBatchGenV2(directory_high_res=val_path,\n",
    "                 image_size_high=(img_height, img_width),\n",
    "                 batch_size=batch_size,\n",
    "                 shuffle=True,\n",
    "                 seed=42,\n",
    "                 data_format=data_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = auto_encoder.fit(data_gen ,\n",
    "                epochs=8,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=True,\n",
    "                validation_data=validator_gen,\n",
    "                callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## draw history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_df = pd.DataFrame(history.history)\n",
    "history_df['epoch'] = history.epoch\n",
    "fig = plt.Figure(figsize=(4, 3))\n",
    "\n",
    "epochs_to_mean = 1\n",
    "\n",
    "plt.plot(history_df['loss'].rolling(epochs_to_mean).mean(), 'b')\n",
    "plt.plot(history_df['val_loss'].rolling(epochs_to_mean).mean(), '-r')\n",
    "plt.title('Learning Curves')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(['traininig loss', 'validation loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## let's see the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testbatch = np.random.randint(0,len(validator_gen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_imgs = auto_encoder.predict(validator_gen[testbatch][0])\n",
    "if data_format=='channels_first':\n",
    "    decoded_imgs = np.transpose(decoded_imgs, (0, 2, 3, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 4\n",
    "plt.figure(figsize=(20, n))\n",
    "\n",
    "for i in range(n):\n",
    "    # display original\n",
    "    ax = plt.subplot(2, n, i+1 )\n",
    "    vraw=validator_gen[testbatch][0]\n",
    "    if data_format=='channels_first':\n",
    "        vraw=np.transpose(vraw, (0, 2, 3, 1))\n",
    "    plt.imshow(vraw[i])\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    # display reconstruction\n",
    "    ax = plt.subplot(2, n, i + n + 1)\n",
    "    plt.imshow(decoded_imgs[i].reshape(img_height,img_width,3))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dePIXELize the original frames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_frames_path=\"data/original_frames\"\n",
    "depixelated_frames_path=\"data/depixelated\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "for img in os.listdir(original_frames_path ):\n",
    "        try:\n",
    "            I = np.asarray(PIL.Image.open(f\"{original_frames_path}/{img}\").resize((img_width,img_height)) ).astype(np.float32)/255.0\n",
    "            if data_format=='channels_first':\n",
    "                image = np.transpose(I, (2, 0, 1))\n",
    "            res=auto_encoder.predict(image.reshape(-1,3,img_height,img_width))\n",
    "            res=np.clip(res,0.0,1.0)\n",
    "            if data_format=='channels_first':\n",
    "                res = np.transpose(res.reshape((3,img_height,img_width)), (1, 2, 0))\n",
    "            plt.imsave(f'{depixelated_frames_path}/{img}', res)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(str(e))\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"models/v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://machinelearningmastery.com/save-load-keras-deep-learning-models/\n",
    "# serialize model to JSON\n",
    "model_json = auto_encoder.to_json()\n",
    "with open(f\"{model_path}/model_{img_width}_{img_height}.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "auto_encoder.save_weights(f\"{model_path}/model_{img_width}_{img_height}.h5\")\n",
    "print(\"Model saved to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
